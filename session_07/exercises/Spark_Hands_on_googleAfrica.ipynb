{"cells":[{"cell_type":"markdown","metadata":{"id":"Qgaeck94a1jM"},"source":["# Install Spark"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"tQq08nU6RtS_"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-71e7-m.us-central1-c.c.src-personal.internal:45039\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Our First Spark example</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f2b3017eb00>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["#If you are using  Data proc comment the file for spark instalattion.\n","# !sudo apt update\n","# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","# #Update spark version for install\n","# !wget -q  https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n","# !tar xf spark-3.3.2-bin-hadoop3.tgz\n","\n","# import os\n","# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n","\n","\n","# !pip install -q findspark\n","# !pip install pyspark\n","\n","# import findspark\n","# findspark.init()\n","# findspark.find()\n","\n","from pyspark.sql import DataFrame, SparkSession\n","\n","\n","\n","spark = SparkSession \\\n","       .builder \\\n","       .appName(\"Our First Spark example\") \\\n","       .getOrCreate()\n","\n","spark"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"PkVW8KPLqOVJ"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cluster-71e7-m.us-central1-c.c.src-personal.internal:45039\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Our First Spark example</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f2b3017eb00>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["spark = SparkSession \\\n","       .builder \\\n","       .appName(\"Our First Spark example\") \\\n","       .getOrCreate()\n","spark"]},{"cell_type":"markdown","metadata":{"id":"At82KKPRbITu"},"source":["# Download the file"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_-I-BMFYWOvG","outputId":"af0614ee-2067-41e9-ebbd-236dc1319634"},"outputs":[{"data":{"text/plain":["('people-with-dups.csv', <http.client.HTTPMessage at 0x7f4fc4207430>)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import urllib\n","url = 'https://drive.google.com/file/d/1BFx9CClKTe_syBuIr5WBi5uuSkZf9wdv/view?usp=sharing'\n","url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n","urllib.request.urlretrieve(url, \"people-with-dups.csv\")\n"]},{"cell_type":"markdown","metadata":{"id":"gQ_zkCenl3mk"},"source":["# Instructions\n","\n","##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) De-Duping Data Lab\n","\n","In this exercise, we're doing ETL on a file we've received from a customer. That file contains data about people, including:\n","\n","* first, middle and last names\n","* gender\n","* birth date\n","* Social Security number\n","* salary\n","\n","But, as is unfortunately common in data we get from this customer, the file contains some duplicate records. Worse:\n","\n","* In some of the records, the names are mixed case (e.g., \"Carol\"), while in others, they are uppercase (e.g., \"CAROL\").\n","* The Social Security numbers aren't consistent either. Some of them are hyphenated (e.g., \"992-83-4829\"), while others are missing hyphens (\"992834829\").\n","\n","The name fields are guaranteed to match, if you disregard character case, and the birth dates will also match. The salaries will match as well,\n","and the Social Security Numbers *would* match if they were somehow put in the same format.\n","\n","Your job is to remove the duplicate records. The specific requirements of your job are:\n","\n","* Remove duplicates. It doesn't matter which record you keep; it only matters that you keep one of them.\n","* Standarize the field ssn with out hyphens.\n","* Create a colum age base on the birth date.\n","\n","\n","\n","Finally, you will write the results as a Parquet file.\n","\n","\n","**Hint:** The initial dataset contains 10,000 records.<br/>\n","The de-duplicated result has 9,976 records.\n"]},{"cell_type":"markdown","metadata":{"id":"FvpORm7WmO-9"},"source":["# Read the file"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Spark_Hands_on_googleAfrica_.ipynb  etc\t\tmedia\t\t      srv\n","Untitled.ipynb\t\t\t    hadoop\tmnt\t\t      sys\n","Untitled1.ipynb\t\t\t    home\topt\t\t      tmp\n","Untitled2.ipynb\t\t\t    lib\t\tpeople-with-dups.csv  usr\n","bin\t\t\t\t    lib32\tproc\t\t      var\n","boot\t\t\t\t    lib64\troot\n","copyright\t\t\t    libx32\trun\n","dev\t\t\t\t    lost+found\tsbin\n","Found 2 items\n","drwxr-xr-x   - root hadoop          0 2023-09-22 17:47 /user/root/.sparkStaging\n","-rw-r--r--   1 root hadoop     712779 2023-09-22 18:28 /user/root/people-with-dups.csv\n"]}],"source":["#load the file into HDFS system\n","# Run this just is are using a data proc cluster\n","!hdfs dfs -put people-with-dups.csv /user/root/people-with-dups.csv\n","!hdfs dfs -ls /user/root/"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"X39uPH1OR8Bi"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#Read data from csv file\n","#Optional read the csv file from a gcp bucket\n","from pyspark.sql.functions import *\n","path = \"people-with-dups.csv\"\n","\n","df = (spark\n","    .read\n","    .option(\"header\", \"true\")\n","    .option(\"inferSchema\", \"true\")\n","    .csv(path))\n"," "]},{"cell_type":"code","execution_count":5,"metadata":{"id":"t26CO3t5WlKn"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+----------+---------+------+-------------------+------+-----------+\n","|firstName|middleName| lastName|gender|          birthDate|salary|        ssn|\n","+---------+----------+---------+------+-------------------+------+-----------+\n","|  Emanuel|   Wallace|   Panton|     M|1988-03-04 00:00:00|101255|935-90-7627|\n","|   Eloisa|     Rubye|Cayouette|     F|2000-06-20 00:00:00|204031|935-89-9009|\n","|    Cathi|  Svetlana|    Prins|     F|2012-12-22 00:00:00| 35895|959-30-7957|\n","|  Mitchel|    Andres|Mozdzierz|     M|1966-05-06 00:00:00| 55108|989-27-8093|\n","|    Angla|     Melba|Hartzheim|     F|1938-07-26 00:00:00| 13199|935-27-4276|\n","|   Rachel|    Marlin|Borremans|     F|1923-02-23 00:00:00| 67070|996-41-8616|\n","| Catarina|  Phylicia|  Dominic|     F|1969-09-29 00:00:00|201021|999-84-8888|\n","|  Antione|     Randy| Hamacher|     M|2004-03-05 00:00:00|271486|917-96-3554|\n","| Madaline|  Shawanda| Piszczek|     F|1996-03-17 00:00:00|183944|963-87-9974|\n","|  Luciano|   Norbert|  Sarcone|     M|1962-12-14 00:00:00| 73069|909-96-1669|\n","+---------+----------+---------+------+-------------------+------+-----------+\n","only showing top 10 rows\n","\n"]}],"source":["#see just 10 record of the data set\n","df.show(10)"]},{"cell_type":"markdown","metadata":{"id":"V9iS3_Muux7v"},"source":[]},{"cell_type":"markdown","metadata":{"id":"UPA4ImypmRQh"},"source":["# Processing\n","You can use the [SQL API reference](https://hyukjin-spark.readthedocs.io/en/latest/reference/pyspark.sql.html) for help"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"y5iiyYIRmXVs"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 6:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+---------+----------+--------+------+-------------------+------+---------+\n","|firstName|middleName|lastName|gender|          birthDate|salary|  ssn_new|\n","+---------+----------+--------+------+-------------------+------+---------+\n","|  Rachele|     Tamar|  Aaberg|     F|1955-04-13 00:00:00| 56069|940134488|\n","|  Rebecca|    Bonnie|Aanderud|     F|1946-02-13 00:00:00|171207|927874203|\n","|   Sammie|   Gillian|  Aarant|     F|1915-06-19 00:00:00|173553|946573500|\n","| Bernardo|     Kasey|  Aavang|     M|1964-03-10 00:00:00|195490|983337353|\n","|    Luigi| Guadalupe|  Abadie|     M|1937-01-21 00:00:00| 53677|923148351|\n","+---------+----------+--------+------+-------------------+------+---------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#Create new name columns with only lower case names\n","dedupedDF = df.withColumn('firstName_lower', lower(col('firstName'))) \\\n","          .withColumn('middleName_lower', lower(col('middleName'))) \\\n","          .withColumn('lastName_lower', lower(col('lastName')))\n","\n","#Create new ssn column without any dashes\n","dedupedDF = dedupedDF.withColumn('ssn_new', regexp_replace('ssn', '-', ''))\n","\n","#Create a new age field\n","\n","#Drop duplicates based only on the new columns, gender, birthDate and salary\n","dedupedDF = dedupedDF.dropDuplicates(['firstName_lower', 'middleName_lower', 'lastName_lower', 'ssn_new', 'gender', 'birthDate', 'salary'])\n","\n","#Drop the old columns\n","dedupedDF = dedupedDF.drop('firstName_lower', 'middleName_lower', 'lastName_lower', 'ssn')\n","\n","\n","# In a diferente DF get and show the avagre salary per year from People betwwen 20 and 55 years old.\n","\n","\n","#See everything is ok with the final df\n","dedupedDF.show(5)\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kvZT_GBl2l63","outputId":"a0384368-0c8c-4b66-8ba1-2b683b157b8c"},"outputs":[{"data":{"text/plain":["9976"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["#See the number of rows\n"]},{"cell_type":"markdown","metadata":{"id":"CUwfDVbjmUD4"},"source":["# Save the file"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"pa-VZ1JQnYTs"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#Write the processed data frame on parquet format\n","#Optional Write the csv file in a gcp bucket\n","path = \"people-deduped.parquet\"\n"]},{"cell_type":"code","execution_count":10,"metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 2 items\r\n","-rw-r--r--   1 root hadoop          0 2023-09-25 20:30 /user/root/people_without_dups/_SUCCESS\r\n","-rw-r--r--   1 root hadoop     334386 2023-09-25 20:29 /user/root/people_without_dups/part-00000-620e74c5-f4c6-4a53-9f63-a0449e2fc248-c000.snappy.parquet\r\n"]}],"source":["#validate if the parque file was writed in the HDFS\n","!hdfs dfs -ls /user/root/people_without_dups"]},{"cell_type":"markdown","metadata":{"id":"JmV_DDyT2pUU"},"source":["# Check your answer"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"hV0AM1Z-2p9s"},"outputs":[],"source":["finalCount = dedupedDF.count()\n","assert finalCount == 9976, \"expected 9976 records in finalDF\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":1}